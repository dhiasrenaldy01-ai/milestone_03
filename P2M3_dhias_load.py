from pyspark.sql import SparkSession
from pymongo import MongoClient

def load_data(input_path: str):
    """
    Load transformed patient data from PySpark DataFrame
    into MongoDB.

    Parameters:
    input_path (str): Path to transformed CSV folder
    """
    print("ğŸš€ Load process started")
    print("ğŸ“‚ Reading transformed data from:", input_path)

    spark = SparkSession.builder \
        .appName("P2M3_Load_Patient_Data") \
        .getOrCreate()

    # Read transformed data (CSV folder)
    df = spark.read.csv(
        input_path,
        header=True,
        inferSchema=True
    )

    row_count = df.count()
    if row_count == 0:
        raise ValueError("âŒ No data to load")

    print("ğŸ“Š Total rows to load:", row_count)

    # Convert Spark DataFrame â†’ list of dict
    document_list = [row.asDict() for row in df.collect()]

    # Connect to MongoDB
    client = MongoClient(
        "mongodb+srv://post:post@dhias-renaldy.cl63raf.mongodb.net/"
    )

    db = client["patient"]
    collection = db["data_patient"]

    # Insert data
    collection.insert_many(document_list)

    print("ğŸ’¾ Data successfully loaded to MongoDB")
    print("âœ… Load process completed")

    spark.stop()


# ===== Main Process =====
if __name__ == "__main__":
    input_folder = "/opt/airflow/data/transform_result_patient_pipeline"

    load_data(input_folder)
